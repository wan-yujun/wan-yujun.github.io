<!DOCTYPE html>
<html lang="zh-CN">
<head>
<script src="https://cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css"/>
<script src="/live2d-widget/autoload.js"></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"wuxin-123.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.9.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"livere","storage":true,"lazyload":false,"nav":null,"activeClass":"livere"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":true,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>
<meta name="description" content="主要学习内容  深度学习基础—线性神经网络，多层感知机 卷积神经网络—LeNet，AlexNet，VGG，Inception，ResNet 循环神经网络—RNN，GRU，LSTM，seq2seq 注意力机制—Attention，Transformer 优化算法—SGD，Momentum，Adam 高性能计算—并行，多GPU，分布式 计算机视觉—目标检测，语义分割 自然语言处理—词">
<meta property="og:type" content="article">
<meta property="og:title" content="预备知识">
<meta property="og:url" content="http://wuxin-123.github.io/%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/">
<meta property="og:site_name" content="羽君">
<meta property="og:description" content="主要学习内容  深度学习基础—线性神经网络，多层感知机 卷积神经网络—LeNet，AlexNet，VGG，Inception，ResNet 循环神经网络—RNN，GRU，LSTM，seq2seq 注意力机制—Attention，Transformer 优化算法—SGD，Momentum，Adam 高性能计算—并行，多GPU，分布式 计算机视觉—目标检测，语义分割 自然语言处理—词">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://wuxin-123.github.io/%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/1.png">
<meta property="og:image" content="http://wuxin-123.github.io/%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/2.png">
<meta property="og:image" content="http://wuxin-123.github.io/%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/3.png">
<meta property="article:published_time" content="2023-10-24T12:09:13.000Z">
<meta property="article:modified_time" content="2023-10-24T12:09:13.000Z">
<meta property="article:author" content="羽君的个人站">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://wuxin-123.github.io/%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/1.png">


<link rel="canonical" href="http://wuxin-123.github.io/%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://wuxin-123.github.io/%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/","path":"预备知识/","title":"预备知识"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>预备知识 | 羽君</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

<a target="_blank" rel="noopener" href="https://github.com/wan-yujun" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">羽君</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">保持热爱，奔赴星海</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9"><span class="nav-number">1.</span> <span class="nav-text">主要学习内容</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="nav-number">2.</span> <span class="nav-text">数据操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%9A%E5%AF%BC%E6%95%B0"><span class="nav-number">3.</span> <span class="nav-text">亚导数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6"><span class="nav-number">4.</span> <span class="nav-text">梯度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%AD%90%E5%88%86%E6%AF%8D%E5%B8%83%E5%B1%80"><span class="nav-number">5.</span> <span class="nav-text">分子&#x2F;分母布局</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="nav-number">6.</span> <span class="nav-text">自动求导</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="羽君的个人站"
      src="/images/personal.gif">
  <p class="site-author-name" itemprop="name">羽君的个人站</p>
  <div class="site-description" itemprop="description">在科技的领域里,只有不断学习和探索,才能跟上时代的步伐</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/wan-yujun" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wan-yujun" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/150703916" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;150703916" rel="noopener" target="_blank"><i class="fa bilibili fa-fw"></i>Bilibili</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/m0_46328473" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;m0_46328473" rel="noopener" target="_blank"><i class="fa csdn fa-fw"></i>CSDN</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://wuxin-123.github.io/%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/personal.gif">
      <meta itemprop="name" content="羽君的个人站">
      <meta itemprop="description" content="在科技的领域里,只有不断学习和探索,才能跟上时代的步伐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="羽君">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          预备知识
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-24 20:09:13" itemprop="dateCreated datePublished" datetime="2023-10-24T20:09:13+08:00">2023-10-24</time>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h4 id="主要学习内容">主要学习内容</h4>
<ul>
<li>深度学习基础—<strong>线性神经网络，多层感知机</strong></li>
<li>卷积神经网络—<strong>LeNet，AlexNet，VGG，Inception，ResNet</strong></li>
<li>循环神经网络—<strong>RNN，GRU，LSTM，seq2seq</strong></li>
<li>注意力机制—<strong>Attention，Transformer</strong></li>
<li>优化算法—<strong>SGD，Momentum，Adam</strong></li>
<li>高性能计算—<strong>并行，多GPU，分布式</strong></li>
<li>计算机视觉—<strong>目标检测，语义分割</strong></li>
<li>自然语言处理—<strong>词嵌入，BERT</strong></li>
</ul>
<h4 id="数据操作">数据操作</h4>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import torch
--------------------------------------------
# numel()函数
x &#x3D; torch.arange(12)
x.numel()	# 返回数组中元素的个数，一定是个标量
# 结果：12
--------------------------------------------
# cat()函数
x &#x3D; torch.arange(4).reshape((2,2))
y &#x3D; torch.arange(4).reshape((2,2))
torch.cat((x,y), dim&#x3D;0)		# 在0维上进行合并
torch.cat((x,y), dim&#x3D;1)		# 在1维上进行合并
# 结果：
(tensor([[0, 1],
         [2, 3],
         [0, 1],
         [2, 3]]),
 tensor([[0, 1, 0, 1],
         [2, 3, 2, 3]]))
--------------------------------------------
# 内存分配问题
x &#x3D; torch.rand(12).reshape(3,4)
y &#x3D; torch.rand(12).reshape(3,4)
before &#x3D; id(x)
x[:] &#x3D; x + y	# 以后尽量写成这种形式，原地操作。还可以这种‘x +&#x3D; y’这种形式，也是原地操作。
# x &#x3D; x + y，这种形式会开辟新内存，不建议。
id(x) &#x3D;&#x3D; before
# 结果：True
--------------------------------------------
# reshape的小坑
a &#x3D; torch.arange(12)
b &#x3D; a.reshape((3,4))
b[:] &#x3D; 1
a
# 结果：
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
# 说明b只是a的一个view，对b进行操作仍然会修改到a的内存。注意！！！
--------------------------------------------
# clone()函数
a &#x3D; torch.arange(12).reshape(3,4)
b &#x3D; a.clone()	# 开辟一个新的内存赋值给b
a[1,2] &#x3D;123
a,b
# 结果：
(tensor([[  0,   1,   2,   3],
         [  4,   5, 123,   7],
         [  8,   9,  10,  11]]),
 tensor([[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]]))
--------------------------------------------
# 哈达玛积	
a &#x3D; torch.arange(12).reshape(3,4)
b &#x3D; a.clone()
a,b,a*b		# a*b就是哈达玛积，也就是点乘的意思。
# 结果：
(tensor([[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]]),
 tensor([[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]]),
 tensor([[  0,   1,   4,   9],
         [ 16,  25,  36,  49],
         [ 64,  81, 100, 121]]))
--------------------------------------------
# 不同维度的sum()函数求和，注意！！！
a &#x3D; torch.arange(20*2).reshape(2,5,4)
b &#x3D; a.sum(axis &#x3D; 0)		# 对第几维求和就是把第几维消掉，这里是第0维。
c &#x3D; a.sum(axis &#x3D; 1)		# 消掉第1维,结果是对列求和。
d &#x3D; a.sum(axis &#x3D; 2)		# 消掉第2维,结果是对行求和。
a
# 结果：
(tensor([[[ 0,  1,  2,  3],
          [ 4,  5,  6,  7],
          [ 8,  9, 10, 11],
          [12, 13, 14, 15],
          [16, 17, 18, 19]],
 
         [[20, 21, 22, 23],
          [24, 25, 26, 27],
          [28, 29, 30, 31],
          [32, 33, 34, 35],
          [36, 37, 38, 39]]]),
b,b.shape
# 结果：
(tensor([[20, 22, 24, 26],
         [28, 30, 32, 34],
         [36, 38, 40, 42],
         [44, 46, 48, 50],
         [52, 54, 56, 58]]),
 torch.Size([5, 4]))
c,c.shape
# 结果：
(tensor([[ 40,  45,  50,  55],
         [140, 145, 150, 155]]),
 torch.Size([2, 4]))
d,d.shape
# 结果：
(tensor([[  6,  22,  38,  54,  70],
         [ 86, 102, 118, 134, 150]]),
 torch.Size([2, 5]))
--------------------------------------------
# 求均值mean()函数，这里a必须是浮点数或者复数。不能是整数，会报错。
a &#x3D; torch.arange(20,dtype &#x3D; torch.float32).reshape(4,5)		#方式一
a &#x3D; torch.arange(20).reshape(4,5)							#方式二
a &#x3D; a.float()
a,a.mean(), a.sum() &#x2F; a.numel()
# 结果：
(tensor([[ 0.,  1.,  2.,  3.,  4.],
         [ 5.,  6.,  7.,  8.,  9.],
         [10., 11., 12., 13., 14.],
         [15., 16., 17., 18., 19.]]),
 tensor(9.5000),
 tensor(9.5000))
--------------------------------------------
# 求和后，不想要丢掉维度，使用keepdims参数为True
a &#x3D; torch.arange(20).reshape(4,5)
b &#x3D; a.sum(axis &#x3D; 0)
c &#x3D; a.sum(axis &#x3D; 0, keepdims &#x3D; True)
b,c
# 结果：
(tensor([30, 34, 38, 42, 46]), tensor([[30, 34, 38, 42, 46]]))	# 可以看到c是二维，而b是一维
--------------------------------------------
# 累加求和cumsum()函数
a &#x3D; torch.arange(20).reshape(4,5)
a,a.cumsum(axis&#x3D;0)			# 第0维累加求和，也就是按照列进行累加求和。
# 结果：
(tensor([[ 0,  1,  2,  3,  4],
         [ 5,  6,  7,  8,  9],
         [10, 11, 12, 13, 14],
         [15, 16, 17, 18, 19]]),
 tensor([[ 0,  1,  2,  3,  4],
         [ 5,  7,  9, 11, 13],
         [15, 18, 21, 24, 27],
         [30, 34, 38, 42, 46]]))
--------------------------------------------
# dot(a,b)点积函数，两者必须相同类型，且必须是一维才能进行点积操作
a &#x3D; torch.arange(5)
b &#x3D; torch.ones(5,dtype&#x3D;torch.long)
a,b,torch.dot(a,b)
# 结果：
(tensor([0, 1, 2, 3, 4]), tensor([1, 1, 1, 1, 1]), tensor(10)) 
--------------------------------------------
# mv(a,b)矩阵向量乘法函数，m代表Matrix，v代表vector。满足线代矩阵乘法定义。
a &#x3D; torch.arange(20).reshape(5,4)
b &#x3D; torch.arange(4)
a,b,torch.mv(a,b)
# 结果：
(tensor([[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11],
         [12, 13, 14, 15],
         [16, 17, 18, 19]]),
 tensor([0, 1, 2, 3]),
 tensor([ 14,  38,  62,  86, 110]))
--------------------------------------------
# mm(a,b)矩阵乘法函数，即真正的矩阵乘法。m代表Matrix。满足线代矩阵乘法定义。
a &#x3D; torch.arange(20).reshape(5,4)
b &#x3D; torch.arange(12).reshape(4,3)
a,b,torch.mm(a,b)
# 结果：
(tensor([[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11],
         [12, 13, 14, 15],
         [16, 17, 18, 19]]),
 tensor([[ 0,  1,  2],
         [ 3,  4,  5],
         [ 6,  7,  8],
         [ 9, 10, 11]]),
 tensor([[ 42,  48,  54],
         [114, 136, 158],
         [186, 224, 262],
         [258, 312, 366],
         [330, 400, 470]]))
--------------------------------------------
# norm()范数。
a &#x3D; torch.tensor([3.,4.])
torch.norm(a)
# 结果：
tensor(5.)
# 具体API调用如下
def norm(input, p&#x3D;&quot;fro&quot;, dim&#x3D;None, keepdim&#x3D;False):
#参数解释：
#input：输入tensor类型的数据
#p：指定的范数
#默认为p&#x3D;‘fro’，计算矩阵的Frobenius norm (Frobenius 范数)，就是矩阵各项元素的绝对值平方的总和。
#p&#x3D;&#39;nuc’时，是求核范数，核范数是矩阵奇异值的和。（不常用）
#p为int的形式，是求p范数。（常用）
#dim：指定在哪个维度进行，如果不指定，则是在所有维度进行计算
#keepdim：True or False，如果True，则保留dim指定的维度，False则不保留
--------------------------------------------
# torch里不区分行向量与列向量，一维张量一定是行向量，也就是一维数组。而列向量就相当于是n*1维的矩阵。
 </code></pre>
<h4 id="亚导数">亚导数</h4>
<p>将导数拓展到不可微的函数。如： <span class="math display">\[
\large 
 y = |x|\\
\large {
\frac{\partial{|x|}}{\partial x}=
\begin{cases}
1&amp;\text{if}\, x&gt;0\\
-1&amp;\text{if}\, x &lt;0\\
a&amp;\text{if}\, x=0\quad a\in{[-1,1]}
\end{cases}}
\]</span> 另一个例子： <span class="math display">\[
\large
\frac {\partial}{\partial x}\text{max}(x,0) = 
\begin{cases}
1&amp;\text {if}\,x&gt;0\\
0&amp;\text {if}\,x&lt;0\\
a&amp;\text {if}\,x=0,&amp;a \in[0,1]
\end{cases}
\]</span></p>
<h4 id="梯度">梯度</h4>
<figure>
<img src="1.png" alt="1"><figcaption>1</figcaption>
</figure>
<p>标量<span class="math inline">\(\large y\)</span>对向量<span class="math inline">\(\large x\)</span>求导是 <span class="math inline">\(\Large \frac {\partial \rm y}{\partial\bf x}=\left[\frac {\partial \rm y}{\partial x_1},\frac {\partial \rm y}{\partial x_2},\cdots,\frac {\partial \rm y}{\partial x_n}, \right]\)</span> 是一个行向量，一些例子如下：</p>
<figure>
<img src="2.png" alt="2"><figcaption>2</figcaption>
</figure>
<p>向量<span class="math inline">\(\large \bf y\)</span>对标量<span class="math inline">\(\large x\)</span>求导是 <span class="math inline">\(\Large \frac {\partial \bf y}{\partial x}=\left[\begin{array}{c} \frac {\partial \rm y_1}{\partial x}\\ \frac {\partial \rm y_2}{\partial x}\\ \vdots\\ \frac {\partial \rm y_m}{\partial x}\end{array} \right]\)</span> 是一个列向量（和分子对应，因此是<strong>分子布局</strong>）。</p>
<p>向量<span class="math inline">\(\large \bf y\)</span>对向量<span class="math inline">\(\large \bf x\)</span>求导是<span class="math inline">\(\Large \frac{\partial \mathbf{y}}{\partial \mathbf{x}}=\left[\begin{array}{c} \frac{\partial y_1}{\partial \mathbf{x}} \\ \frac{\partial y_2}{\partial \mathbf{x}} \\ \vdots \\ \frac{\partial y_m}{\partial \mathbf{x}} \end{array}\right]=\left[\begin{array}{c} \frac{\partial y_1}{\partial x_1}, \frac{\partial y_1}{\partial x_2}, \ldots, \frac{\partial y_1}{\partial x_n} \\ \frac{\partial y_2}{\partial x_1}, \frac{\partial y_2}{\partial x_2}, \ldots, \frac{\partial y_2}{\partial x_n} \\ \vdots \\ \frac{\partial y_m}{\partial x_1}, \frac{\partial y_m}{\partial x_2}, \ldots, \frac{\partial y_m}{\partial x_n} \end{array}\right]\)</span> 是一个矩阵，一些例子如下。</p>
<figure>
<img src="3.png" alt="3"><figcaption>3</figcaption>
</figure>
<h4 id="分子分母布局">分子/分母布局</h4>
<p>不同的布局方法，对矩阵求偏导后所得的结果不同。例如： <span class="math display">\[
\large
\begin{align}
\bf y &amp;= \bf {Ax} \qquad \bf y = \bf {x^TA} \\
\frac {\partial \bf y}{\partial \bf x} &amp;= A\quad\ \ \ \frac {\partial \bf y}{\partial \bf x} = A^T\quad \bf(分子布局)\\
\frac {\partial \bf y}{\partial \bf x} &amp;= A^T\quad \frac {\partial \bf y}{\partial \bf x} = A\quad \bf(分母布局)
\end{align}
\]</span></p>
<h4 id="自动求导">自动求导</h4>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import torch
x &#x3D; torch.arange(4.0)
x.requires_grad_(True)	# 开启存储梯度的地方，等价于 x &#x3D; torch.arange(4.0, requires_grad &#x3D; True)
y &#x3D; 2 * torch.dot(x,x)	# x与x做内积，这里的y是一个标量即：y &#x3D; 2(x1^2 + x2^2 + x3^2 + x4^2)
y.backward()	# 求导，自动计算x的所有梯度
x.grad			# 所有梯度值会自动累积到.grad属性中
# 结果：tensor([0., 4., 8., 12.])
x.grad.zero_()	# 在默认情况下，PyTorch会自动累积梯度值，因此我们需要用这个方法清空之前的值
y &#x3D; x.sum()		# 构造一个新的函数，这里的y也是标量，即：y &#x3D; x1 + x2 + x3 + x4
y.backward()
x.grad
# 结果：tensor([1., 1., 1., 1.])</code></pre>
<p>在深度学习中，目标函数<span class="math inline">\(\large y\)</span>一般都是标量，而不是向量或矩阵。因此我们所要求的损失函数或者目标函数往往是一个标量而不是向量或矩阵。举一个例子就很好理解了。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import torch
x &#x3D; torch.arange(4.0)
y &#x3D; x * x 	# 这里是点乘，因此y是一个向量,和下面的y.sum()不同</code></pre>
<p>即 <span class="math inline">\(\large y = [x_1^2,x_2^2,x_3^2,x_4^2]\)</span> <span class="math display">\[
如果直接求\frac {\partial \bf y}{\partial \bf x}\text的话，那么推导如下：\\
\Large
\frac{\partial \bf y}{\partial \bf x}=\left[\begin{array}{l}
\frac{\partial y_1}{\partial \bf x} \\
\frac{\partial y_2}{\partial \bf x} \\
\frac{\partial y_3}{\partial \bf x} \\
\frac{\partial y_4}{\partial \bf x}
\end{array}\right]=\left[\begin{array}{llll}
\frac{\partial y_1}{\partial x_1} &amp; \frac{\partial y_1}{\partial x_2} &amp; \frac{\partial y_1}{\partial x_3} &amp; \frac{\partial y_1}{\partial y_4} \\
\frac{\partial y_2}{\partial x_1} &amp; \frac{\partial y_2}{\partial x_2} &amp; \frac{\partial y_2}{\partial x_3} &amp; \frac{\partial y_2}{\partial x_4} \\
\frac{\partial y_3}{\partial x_1} &amp; \frac{\partial y_3}{\partial x_2} &amp; \frac{\partial y_3}{\partial x_3} &amp; \frac{\partial y_3}{\partial x_4} \\
\frac{\partial y_4}{\partial x_1} &amp; \frac{\partial y_4}{\partial x_2} &amp; \frac{\partial y_4}{\partial x_3} &amp; \frac{\partial y_4}{\partial x_4}
\end{array}\right]=\left[\begin{array}{llll}
2x_1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 2x_2 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 2x_3 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 2x_4
\end{array}\right]
\]</span> 这样求出的结果不是我们想要的，我们需要的是将偏导数累加起来求和。因此我们需要对<span class="math inline">\(\large y\)</span>进行先求和再微分。即：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import torch
x &#x3D; torch.arange(4.0)
y &#x3D; x * x 	# 这里是点乘，因此y是一个向量
res &#x3D; y.sum()	# 这里对y进行求和。效果如下所示。这是关键！！！
res.backward()	# 求偏导
x.grad
# 结果:
tensor([0., 2., 4., 6.])</code></pre>
<p>即<span class="math inline">\(\large y = x_1^2 + x_2^2 + x_3^2 + x_4^2\)</span>，这里的<span class="math inline">\(\large y\)</span>就是一个标量。那么我们再对<span class="math inline">\(\large \bf x\)</span>求偏导。即： <span class="math display">\[
\Large
\frac{\partial y}{\partial \bf x}=\left[\begin{array}{l}
\frac{\partial y}{\partial \bf x_1},
\frac{\partial y}{\partial \bf x_2},
\frac{\partial y}{\partial \bf x_3}, 
\frac{\partial y}{\partial \bf x_4},
\end{array}\right]=\left[\begin{array}{l}2x_1,2x_2,2x_3,2x_4,
\end{array}\right]=\left[\begin{array}{l}0,2,4,6
\end{array}\right]
\]</span></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"># detach()函数
x.grad.zero_()	# 对x的偏导数值清空
y &#x3D; x * x 		# y是一个向量
u &#x3D; y.detach()	# 这个函数的作用是将tensor从计算图中分离出来，返回一个新的tensor。也就是说u不再是关于x的函数向量，而是一个向量常数，值为x*x。
z &#x3D; u * x 		# 其中u是常数向量，x是变量向量。z仍然是一个向量。
z.sum().backward()	# z求和之后变成标量了，再进行求偏导
x.grad &#x3D;&#x3D; u
# 结果：
tensor([True, True, True, True])</code></pre>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/markdown%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%B3%95%E4%B8%8E%E4%BE%8B%E5%AD%90/" rel="prev" title="markdown常用语法与例子">
                  <i class="fa fa-chevron-left"></i> markdown常用语法与例子
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="lv-container" data-id="city" data-uid="MTAyMC81ODk0NC8zNTQwNg=="></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">羽君的个人站</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">5k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">17 分钟</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>--><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>


    </div>
  </footer>

  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.25.0/components/prism-core.min.js" integrity="sha256-vlRYHThwdq55dA+n1BKQRzzLwFtH9VINdSI68+5JhpU=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.25.0/plugins/autoloader/prism-autoloader.min.js" integrity="sha256-cl5LNmPvcRcGFaQFjwEKIfs51AX7wBkvoByH6LTxQCs=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.25.0/plugins/line-numbers/prism-line-numbers.min.js" integrity="sha256-K837BwIyiXo5k/9fCYgqUyA14bN4/Ve9P2SIT0KmZD0=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script src="/js/third-party/comments/livere.js"></script>





</body>


</html>
